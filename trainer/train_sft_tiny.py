import os
import sys

__package__ = "trainer"
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import argparse
import time
import warnings
import torch
import torch.distributed as dist
from contextlib import nullcontext
from torch import optim, nn
from torch.nn.parallel import DistributedDataParallel
from torch.utils.data import DataLoader, DistributedSampler
from transformers import AutoTokenizer
from model.model_vlm import TinyMindVLM, VLMConfig
from dataset.lm_dataset import Objects365Dataset
from trainer.trainer_utils import get_lr, Logger, is_main_process, init_distributed_mode, setup_seed, init_tiny_vlm_model, vlm_checkpoint, SkipBatchSampler
import open_clip
warnings.filterwarnings('ignore')

all_images = [x.path for x in os.scandir("/lh/hulk/coco128/images/train2017/") if x.name.endswith("jpg")]
def train_epoch(epoch, loader, iters, start_step=0, wandb=None, tokenizer=None):
    loss_fct = nn.CrossEntropyLoss(reduction='none')
    start_time = time.time()
    for step, (X, Y, loss_mask, pixel_values) in enumerate(loader, start=start_step + 1):
        X = X.to(args.device)
        Y = Y.to(args.device)
        loss_mask = loss_mask.to(args.device)
        pixel_values = pixel_values.to(args.device)
        
        lr = get_lr(epoch * iters + step, args.epochs * iters, args.learning_rate)
        for param_group in optimizer.param_groups:
            param_group['lr'] = lr

        with autocast_ctx:
            res = model(X, pixel_values=pixel_values)
            loss = loss_fct(
                res.logits.view(-1, res.logits.size(-1)),
                Y.view(-1)
            ).view(Y.size())

            loss = (loss * loss_mask).sum() / loss_mask.sum()
            loss += res.aux_loss
            loss = loss / args.accumulation_steps

        scaler.scale(loss).backward()

        if (step + 1) % args.accumulation_steps == 0:
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)

            scaler.step(optimizer)
            scaler.update()

            optimizer.zero_grad(set_to_none=True)

        if step % args.log_interval == 0 or step == iters - 1:
            spend_time = time.time() - start_time
            current_loss = loss.item() * args.accumulation_steps
            current_lr = optimizer.param_groups[-1]['lr']
            eta_min = spend_time / (step + 1) * iters // 60 - spend_time // 60
            
            Logger(f'Epoch:[{epoch+1}/{args.epochs}]({step}/{iters}) loss:{current_loss:.6f} lr:{current_lr:.12f} epoch_Time:{eta_min}min:')
            
            if wandb: wandb.log({"loss": current_loss, "lr": current_lr, "epoch_Time": eta_min})

        if (step % args.save_interval == 0 or step == iters - 1) and is_main_process():
            model.eval()
            moe_suffix = '_moe' if vlm_config.use_moe else ''
            ckp = f'{args.save_dir}/{args.save_weight}_{vlm_config.hidden_size}{moe_suffix}_epoch{epoch}.pth'
            if isinstance(model, torch.nn.parallel.DistributedDataParallel):
                state_dict = model.module.state_dict()
            else:
                state_dict = model.state_dict()
            clean_state_dict = {
                key: value for key, value in state_dict.items() #if not key.startswith('vision_encoder.')
            }
            # clean_state_dict = {k: v.half() for k, v in clean_state_dict.items()}  # ÂçäÁ≤æÂ∫¶‰øùÂ≠ò
            clean_state_dict = {k: v for k, v in clean_state_dict.items()}  # ÂçäÁ≤æÂ∫¶‰øùÂ≠ò
            torch.save(clean_state_dict, ckp)
            vlm_checkpoint(vlm_config, weight=args.save_weight, model=model, optimizer=optimizer, 
                         epoch=epoch, step=step, wandb=wandb, save_dir='../checkpoints', scaler=scaler)
            

            from PIL import Image
            import random
            import numpy as np
            from dataset.template import prompts_template
            image_path = random.choice(all_images)
            query = random.choice(prompts_template)
            image = Image.open(image_path).convert('RGB')

            pixel_values = TinyMindVLM.image2tensor(image, preprocess).to(args.device).unsqueeze(0)

            messages = [
                {"role": "system", "content": query},
                {
                    "role": "user",
                    "content":  f"{'@'*49}ÁúãËøôÂº†ÂõæÁâáÔºåËØ¥ËØ¥‰Ω†ÁúãÂà∞ÁöÑ„ÄÇ"
                    
                }
            ]
            inputs_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            inputs = tokenizer(inputs_text, return_tensors="pt", truncation=True).to(args.device)
            
            print(f'üë∂: {messages}')
            print('ü§ñÔ∏è: ', end='')
            output = model.generate(
                inputs=inputs["input_ids"], attention_mask=inputs["attention_mask"],
                max_new_tokens=1024, do_sample=True, 
                pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id,
                top_p=0.85, temperature=0.65, pixel_values=pixel_values
            )

            decoded_text = tokenizer.decode(
                output[0], 
                skip_special_tokens=True, 
                clean_up_tokenization_spaces=True
            )

            decoded_text = decoded_text.replace("@" * 49, "<image>")
            print('\n\n')

            table = wandb.echarts.Table()
            headers = ["ËæìÂÖ•ÈóÆÈ¢ò", "Ê®°ÂûãËæìÂá∫"]
            rows = [[query, decoded_text]]
            table.add(headers, rows)

            wandb.log(
                {
                    "sample/ËæìÂÖ•ÂõæÂÉè": wandb.Image(np.array(image)),     
                    "sample/ÈóÆÈ¢ò&ÂõûÂ§ç": table,                      
                }
            )

            model.train()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="MomiMindVLM SFT")
    parser.add_argument("--save_dir", type=str, default="../out", help="Ê®°Âûã‰øùÂ≠òÁõÆÂΩï")
    parser.add_argument('--save_weight', default='sft_vlm', type=str, help="‰øùÂ≠òÊùÉÈáçÁöÑÂâçÁºÄÂêç")
    parser.add_argument("--epochs", type=int, default=2, help="ËÆ≠ÁªÉËΩÆÊï∞")
    parser.add_argument("--batch_size", type=int, default=4, help="batch size")
    parser.add_argument("--learning_rate", type=float, default=1e-6, help="ÂàùÂßãÂ≠¶‰π†Áéá")
    parser.add_argument("--device", type=str, default="cuda:0" if torch.cuda.is_available() else "cpu", help="ËÆ≠ÁªÉËÆæÂ§á")
    parser.add_argument("--dtype", type=str, default="bfloat16", help="Ê∑∑ÂêàÁ≤æÂ∫¶Á±ªÂûã")
    parser.add_argument("--num_workers", type=int, default=8, help="Êï∞ÊçÆÂä†ËΩΩÁ∫øÁ®ãÊï∞")
    parser.add_argument("--accumulation_steps", type=int, default=1, help="Ê¢ØÂ∫¶Á¥ØÁßØÊ≠•Êï∞")
    parser.add_argument("--grad_clip", type=float, default=1.0, help="Ê¢ØÂ∫¶Ë£ÅÂâ™ÈòàÂÄº")
    parser.add_argument("--log_interval", type=int, default=10, help="Êó•ÂøóÊâìÂç∞Èó¥Èöî")
    parser.add_argument("--save_interval", type=int, default=1000, help="Ê®°Âûã‰øùÂ≠òÈó¥Èöî")
    parser.add_argument('--hidden_size', default=512, type=int, help="ÈöêËóèÂ±ÇÁª¥Â∫¶")
    parser.add_argument('--num_hidden_layers', default=8, type=int, help="ÈöêËóèÂ±ÇÊï∞Èáè")
    parser.add_argument('--max_seq_len', default=1536, type=int, help="ËÆ≠ÁªÉÁöÑÊúÄÂ§ßÊà™Êñ≠ÈïøÂ∫¶")
    parser.add_argument('--use_moe', default=0, type=int, choices=[0, 1], help="ÊòØÂê¶‰ΩøÁî®MoEÊû∂ÊûÑÔºà0=Âê¶Ôºå1=ÊòØÔºâ")
    parser.add_argument("--annotation_file", type=str, default="../dataset/sft_data.jsonl", help="ËÆ≠ÁªÉÊï∞ÊçÆË∑ØÂæÑ")
    parser.add_argument("--data_path", type=str, default="../dataset/sft_data.jsonl", help="ËÆ≠ÁªÉÊï∞ÊçÆË∑ØÂæÑ")
    parser.add_argument("--images_path", type=str, default="../dataset/sft_images", help="ËÆ≠ÁªÉÂõæÂÉèË∑ØÂæÑ")
    parser.add_argument('--from_weight', default='none', type=str, help="Âü∫‰∫éÂì™‰∏™ÊùÉÈáçËÆ≠ÁªÉÔºå‰∏∫noneÂàô‰∏çÂü∫‰∫é‰ªª‰ΩïÊùÉÈáçËÆ≠ÁªÉ")
    parser.add_argument('--from_resume', default=0, type=int, choices=[0, 1], help="ÊòØÂê¶Ëá™Âä®Ê£ÄÊµã&Áª≠ËÆ≠Ôºà0=Âê¶Ôºå1=ÊòØÔºâ")
    parser.add_argument("--use_wandb", action="store_true", help="ÊòØÂê¶‰ΩøÁî®wandb")
    parser.add_argument("--wandb_project", type=str, default="MomiMindVLM-SFT", help="wandbÈ°πÁõÆÂêç")
    args = parser.parse_args()

    # ========== 1. ÂàùÂßãÂåñÁéØÂ¢ÉÂíåÈöèÊú∫ÁßçÂ≠ê ==========
    local_rank = init_distributed_mode()
    if dist.is_initialized(): args.device = f"cuda:{local_rank}"
    setup_seed(42 + (dist.get_rank() if dist.is_initialized() else 0))
    
    # ========== 2. ÈÖçÁΩÆÁõÆÂΩï„ÄÅÊ®°ÂûãÂèÇÊï∞„ÄÅÊ£ÄÊü•ckp ==========
    os.makedirs(args.save_dir, exist_ok=True)

    vlm_config =  VLMConfig(ve_hidden_size=679, hidden_size=512, num_hidden_layers=8, use_moe=False,
                            image_special_token = '@' * 49,
                            image_ids = [34] * 49,)
    
    # ========== 3. ËÆæÁΩÆÊ∑∑ÂêàÁ≤æÂ∫¶ ==========
    device_type = "cuda" if "cuda" in args.device else "cpu"
    dtype = torch.bfloat16 if args.dtype == "bfloat16" else torch.float16
    autocast_ctx = nullcontext() if device_type == "cpu" else torch.amp.autocast("cuda", dtype=dtype) #torch.cuda.amp.autocast(dtype=dtype)
    
    # ========== 4. ÈÖçwandb ==========
    wandb = None
    if args.use_wandb and is_main_process():
        import swanlab as wandb
        wandb_id = None
        resume = 'must' if wandb_id else None
        wandb_run_name = f"MomiMindVLM-SFT-Epoch-{args.epochs}-BatchSize-{args.batch_size}-LearningRate-{args.learning_rate}"
        wandb.init(project=args.wandb_project, name=wandb_run_name, id=wandb_id, resume=resume, mode="offline", logdir="./swanlog/tiny2")
    
    # ========== 5. ÂÆö‰πâÊ®°Âûã„ÄÅÊï∞ÊçÆ„ÄÅ‰ºòÂåñÂô® ==========
    model, tokenizer, preprocess = init_tiny_vlm_model(vlm_config, 
                                                   device=args.device, tokenizer_path="/lh/hulk/MiniMind2-Small", 
                                                   vision_model_path='/lh/hulk/Cream-0ef394cd0c0f41b55fb073ab9abbb95acc13104e/TinyCLIP-auto-ViT-63M-32-Text-31M-LAIONYFCC400M.pt', 
                                                   )
    for name, param in model.named_parameters():
        if 'vision_encoder' in name:
            param.requires_grad = False
        # if 'vision_encoder' in name:
        #     if "final_conv" in name or "vision_encoder.1.3" in name:
        #          param.requires_grad = True
        # if 'vision_proj' not in name:
        #     param.requires_grad = False

    Logger(f'ÊâÄÂä†ËΩΩVLM ModelÂèØËÆ≠ÁªÉÂèÇÊï∞Ôºö{sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.3f} MB')
    
    train_ds = Objects365Dataset(annotation_file = args.annotation_file, 
                                root_dir = args.data_path, tokenizer = tokenizer, preprocess=preprocess,
                                image_special_token=vlm_config.image_special_token,
                                max_length=vlm_config.max_seq_len)
    
    train_sampler = DistributedSampler(train_ds) if dist.is_initialized() else None
    # scaler = torch.cuda.amp.GradScaler(enabled=(args.dtype == 'float16'))
    scaler = torch.amp.GradScaler('cuda', enabled=(args.dtype == 'float16'))
    optimizer = optim.AdamW(model.parameters(), lr=args.learning_rate)
    
    # ========== 6. ‰ªéckpÊÅ¢Â§çÁä∂ÊÄÅ ==========
    start_epoch, start_step = 0, 0
   
    
    # ========== 7. DDPÂåÖÊ®°Âûã ==========
    if dist.is_initialized():
        model._ddp_params_and_buffers_to_ignore = {"pos_cis"}
        model = DistributedDataParallel(model, device_ids=[local_rank])
    
    # ========== 8. ÂºÄÂßãËÆ≠ÁªÉ ==========
    for epoch in range(start_epoch, args.epochs):
        train_sampler and train_sampler.set_epoch(epoch)
        if epoch == start_epoch and start_step > 0: # Á¨¨‰∏Ä‰∏™epoch‰∏îÂ≠òÂú®Ê£ÄÊü•ÁÇπ
            batch_sampler = SkipBatchSampler(train_sampler or range(len(train_ds)), args.batch_size, start_step + 1)
            loader = DataLoader(train_ds, batch_sampler=batch_sampler, num_workers=args.num_workers, pin_memory=True)
            Logger(f'Epoch [{epoch + 1}/{args.epochs}]: Ë∑≥ËøáÂâç{start_step}‰∏™stepÔºå‰ªéstep {start_step + 1}ÂºÄÂßã')
            train_epoch(epoch, loader, len(loader) + start_step + 1, start_step, wandb, tokenizer)
        else: # ÈªòËÆ§‰ªéÂ§¥ÂºÄÂßã
            loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=(train_sampler is None), sampler=train_sampler, num_workers=args.num_workers, pin_memory=True)
            train_epoch(epoch, loader, len(loader), 0, wandb, tokenizer)

#  python trainer/train_sft_vlm2.py --annotation_file /objects365/val/zhiyuan_objv2_captions_x512_val.json --data_path /objects365/val/ --save_dir exp_proj --num_workers 16 --batch_size 2 --use_wand
            
#  python trainer/train_sft_vlm2.py --annotation_file /objects365/train/zhiyuan_objv2_captions_x512_train.json --data_path /objects365/train/ --save_dir exp3 --num_workers 16 --batch_size 32 --use_wand --learning_rate 1e-6 --epochs 3 --wandb_project MomiMindVLM3
            

# python trainer/train_sft_tiny.py --annotation_file /objects365/train/zhiyuan_objv2_captions_x512_train.json --data_path /objects365/train/ --save_dir exp_tiny1 --num_workers 16 --batch_size 32 --use_wand --learning_rate 1e-6 --epochs 1 --wandb_project TinyMindVLM1